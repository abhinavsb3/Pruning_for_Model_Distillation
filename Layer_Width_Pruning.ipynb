{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5NDODr8HMfEWIAt7495Gd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhinavsb3/Pruning_for_Model_Distillation/blob/main/Layer_Width_Pruning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Layer Pruning with naive Width Pruning**"
      ],
      "metadata": {
        "id": "FGUAe9K1cC43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install --upgrade pip -q\n",
        "!pip install transformers -qU"
      ],
      "metadata": {
        "id": "fgsNa8bCulyb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5db2b9c-fb36-4354-d859-2d1c9128a184"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-NTJ6ht4bgce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3821258c-e5a9-4012-d0c6-c3ca4ab0c348"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "____________________________________\n",
            "Original Model:\n",
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(49152, 576, padding_idx=2)\n",
            "    (layers): ModuleList(\n",
            "      (0-29): 30 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
            "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
            "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
            "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
            "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
            "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
            "          (act_fn): SiLUActivation()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
            ")\n",
            "____________________________________\n",
            "Original model Parameters: 134515008\n",
            "____________________________________\n",
            "\n",
            "Model parameters after layer pruning: 116,814,528\n",
            "____________________________________\n",
            "\n",
            "Model parameters after hidden dimension pruning: 93,457,728\n",
            "____________________________________\n",
            "\n",
            "Total size reduction: 30.52 %\n",
            "____________________________________\n",
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(49152, 576, padding_idx=2)\n",
            "    (layers): ModuleList(\n",
            "      (0-24): 25 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
            "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
            "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
            "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
            "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
            "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
            "          (act_fn): SiLUActivation()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
            ")\n",
            "____________________________________\n",
            "\n",
            "prunedLM-100M-Instruct saved to: SmolLM-135M-Instruct-layer-width-pruned-90000000M-raw\n",
            "____________________________________\n",
            "--- Actual pruned Shapes ---\n",
            "Q-Proj shape: torch.Size([504, 504])\n",
            "K-Proj shape: torch.Size([192, 168])\n",
            "V-Proj shape: torch.Size([192, 168])\n",
            "--------------------\n",
            "MLP Gate-Proj shape: torch.Size([1344, 504])\n",
            "MLP Down-Proj shape: torch.Size([504, 1344])\n",
            "____________________________________\n",
            "LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"dtype\": \"float32\",\n",
            "  \"eos_token_id\": 2,\n",
            "  \"head_dim\": 64,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 504,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1344,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 9,\n",
            "  \"num_hidden_layers\": 25,\n",
            "  \"num_key_value_heads\": 3,\n",
            "  \"pad_token_id\": 2,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"transformers_version\": \"4.57.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 49152\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "import torch\n",
        "import math\n",
        "\n",
        "def total_parameters(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def prune_layers(model, target_params, original_params):\n",
        "  total_layers = len(model.model.layers)\n",
        "\n",
        "  #Calculate number of layers to keep\n",
        "  layers_to_keep = round((target_params / original_params) * total_layers)\n",
        "  layers_to_prune = total_layers - layers_to_keep\n",
        "\n",
        "  #Keep all layers except those right before the last layer\n",
        "  final_layers = (\n",
        "      list(model.model.layers[:total_layers - layers_to_prune - 1] +\n",
        "           [model.model.layers[-1]])\n",
        "  )\n",
        "\n",
        "  model.model.layers = torch.nn.ModuleList(final_layers)\n",
        "\n",
        "  model.config.num_hidden_layers = len(final_layers)\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def prune_hidden_dimensions(model, target_params, current_params):\n",
        "  original_hidden_size = model.config.hidden_size\n",
        "  original_intermediate_size = model.config.intermediate_size\n",
        "  original_proj_ratio = original_intermediate_size / original_hidden_size #Calculate ratio dynamically\n",
        "  num_heads = model.config.num_attention_heads\n",
        "\n",
        "  #Estimate new hidden size to taget parameters\n",
        "  reduction_ratio  = math.sqrt(target_params / current_params)\n",
        "  new_hidden_size = int(original_hidden_size * reduction_ratio)\n",
        "  new_hidden_size = (new_hidden_size // (2 * num_heads)) * (2 * num_heads) #Ensure divisibilty\n",
        "\n",
        "  num_attention_heads = model.config.num_attention_heads\n",
        "  num_key_value_heads = model.config.num_key_value_heads\n",
        "\n",
        "  #Update hidden size and intermediate size in the config\n",
        "  model.config.hidden_size = new_hidden_size\n",
        "  model.config.intermediate_size = int(new_hidden_size * original_proj_ratio) #Maintain the original ratio\n",
        "\n",
        "  for layer in model.model.layers:\n",
        "    #Adjust attention projection layers\n",
        "    layer.self_attn.q_proj.weight = torch.nn.Parameter(\n",
        "        layer.self_attn.q_proj.weight[:new_hidden_size, :new_hidden_size].contiguous()\n",
        "    )\n",
        "    layer.self_attn.k_proj.weight = torch.nn.Parameter(\n",
        "        layer.self_attn.k_proj.weight[:new_hidden_size, :new_hidden_size // (num_attention_heads//num_key_value_heads)].contiguous()\n",
        "    )\n",
        "    layer.self_attn.v_proj.weight = torch.nn.Parameter(\n",
        "        layer.self_attn.v_proj.weight[:new_hidden_size, :new_hidden_size // (num_attention_heads//num_key_value_heads)].contiguous()\n",
        "    )\n",
        "    layer.self_attn.o_proj.weight = torch.nn.Parameter(\n",
        "        layer.self_attn.o_proj.weight[:new_hidden_size, :new_hidden_size].contiguous()\n",
        "    )\n",
        "\n",
        "    #Adjust MLP layers\n",
        "    new_intermediate_size = model.config.intermediate_size\n",
        "    layer.mlp.gate_proj.weight = torch.nn.Parameter(\n",
        "        layer.mlp.gate_proj.weight[:new_intermediate_size, :new_hidden_size].contiguous()\n",
        "    )\n",
        "    layer.mlp.up_proj.weight = torch.nn.Parameter(\n",
        "        layer.mlp.up_proj.weight[:new_intermediate_size, :new_hidden_size].contiguous()\n",
        "    )\n",
        "    layer.mlp.down_proj.weight = torch.nn.Parameter(\n",
        "        layer.mlp.down_proj.weight[:new_hidden_size, :new_intermediate_size].contiguous()\n",
        "    )\n",
        "\n",
        "    #Adjust rotary positional embeddings\n",
        "    rotary_dim = new_hidden_size // num_heads\n",
        "    model.model.rotary_emb_inv_freq = model.model.rotary_emb.inv_freq[:rotary_dim].contiguous()\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def create_pruned_lm(model_name, target_params_1, target_params_2):\n",
        "  #Step 1 :Load the model\n",
        "  model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "  print(\"____________________________________\")\n",
        "  print(\"Original Model:\")\n",
        "  print(model)\n",
        "\n",
        "  #Count original parameters\n",
        "  original_params = total_parameters(model)\n",
        "  print(\"____________________________________\")\n",
        "  print(f\"Original model Parameters: {original_params}\")\n",
        "\n",
        "  #Step 2 :Prune layers to target ~90M parameters\n",
        "  model = prune_layers(model, target_params_1, original_params)\n",
        "\n",
        "  new_params = total_parameters(model)\n",
        "  print(\"____________________________________\")\n",
        "  print(f\"\\nModel parameters after layer pruning: {new_params:,}\")\n",
        "\n",
        "  #Step 3 :Prune hidden dimensions to target again reduce parameters size\n",
        "  model = prune_hidden_dimensions(model, target_params_2, new_params)\n",
        "\n",
        "  final_params = total_parameters(model)\n",
        "  print(\"____________________________________\")\n",
        "  print(f\"\\nModel parameters after hidden dimension pruning: {final_params:,}\")\n",
        "\n",
        "  print(\"____________________________________\")\n",
        "  reduction_percentage = (1 - final_params / original_params) * 100\n",
        "  print(f\"\\nTotal size reduction: {reduction_percentage:.2f} %\")\n",
        "\n",
        "  print(\"____________________________________\")\n",
        "  print(model)\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "model_name = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
        "target_params_1 = 110_000_000\n",
        "target_params_2 = 90_000_000\n",
        "\n",
        "#Create the pruned model\n",
        "pruned_lm = create_pruned_lm(model_name, target_params_1, target_params_2)\n",
        "\n",
        "modified_model_path = f\"{model_name.split('/')[1]}-layer-width-pruned-{int(target_params_2)}M-raw\"\n",
        "pruned_lm.save_pretrained(modified_model_path)\n",
        "print(\"____________________________________\")\n",
        "print(f\"\\nprunedLM-100M-Instruct saved to: {modified_model_path}\")\n",
        "print(\"____________________________________\")\n",
        "# Check the first layer of your pruned model\n",
        "layer_to_inspect = pruned_lm.model.layers[0]\n",
        "\n",
        "print(\"--- Actual pruned Shapes ---\")\n",
        "\n",
        "# Attention block weights\n",
        "print(\"Q-Proj shape:\", layer_to_inspect.self_attn.q_proj.weight.shape)\n",
        "print(\"K-Proj shape:\", layer_to_inspect.self_attn.k_proj.weight.shape)\n",
        "print(\"V-Proj shape:\", layer_to_inspect.self_attn.v_proj.weight.shape)\n",
        "\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# MLP block weights\n",
        "print(\"MLP Gate-Proj shape:\", layer_to_inspect.mlp.gate_proj.weight.shape)\n",
        "print(\"MLP Down-Proj shape:\", layer_to_inspect.mlp.down_proj.weight.shape)\n",
        "print(\"____________________________________\")\n",
        "print(pruned_lm.config)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IhCL7Es_4Mp9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}