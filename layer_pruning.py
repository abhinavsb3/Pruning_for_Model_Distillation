# -*- coding: utf-8 -*-
"""Layer_pruning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ySaF0NVPVQqOObm4oL-q5gtNw-O1MXp1
"""



!python -m pip install --upgrade pip -q
!pip install transformers -qU

from transformers import AutoModelForCausalLM

#Model = "HuggingFaceTB/SmolLM-360M-instruct"
#Model = "HuggingFaceTB/SmolLM-135M-instruct"

#Student Model
model_name = "HuggingFaceTB/SmolLM-135M"

model = AutoModelForCausalLM.from_pretrained(model_name)

print("original_model:")
print(model)

"""**LAYER PRUNING**"""

import torch
import math

def total_parameters(model):
  return sum(p.numel() for p in model.parameters() if p.requires_grad)

def create_pruned_lm(model_name, target_params):
  model = AutoModelForCausalLM.from_pretrained(model_name)

  print("Original_model:")
  print(model)

  original_params = total_parameters(model)
  print(f"Original model parameters: {original_params}")

  total_layers = len(model.model.layers)

  #Calculate number of layers to keep
  layers_to_keep = round((target_params / original_params) * total_layers)
  layers_to_prune = total_layers - layers_to_keep

  #Keep all layers except those right before the last layer
  final_layers = (
      list(model.model.layers[:total_layers - layers_to_prune - 1] +
           [model.model.layers[-1]])
  )

  model.model.layers = torch.nn.ModuleList(final_layers)

  model.config.num_hidden_layers = len(final_layers)

  print(".\nModifed model (PrunedLM)")
  print(model)

  new_params = total_parameters(model)
  print(f"The new pruned model parameters: {new_params:,}")

  reduction_percentage = (1 - new_params / original_params) * 100
  print(f"Size reduction: {reduction_percentage:.2f}%")

  return model


model_name = "HuggingFaceTB/SmolLM-135M"
target_params = 90_000_000

pruned_lm = create_pruned_lm(model_name, target_params)

modified_model_path = f"{model_name.split('/')[1]}-layer-pruned-{int(target_params/1000000)}M-raw"
pruned_lm.save_pretrained(modified_model_path)
print(f"\nTrelisLM-100M-Instruct saved to: {modified_model_path}")

"""Push the Model to Hub

"""

from huggingface_hub import login
login()
## my token

from transformers import AutoModelForCausalLM, AutoTokenizer

def push_model_to_hub(local_model_path, original_model_name, repo_name):
  #Using the same tokenizer as original model
  tokenizer = AutoTokenizer.from_pretrained(original_model_name)

  #Load the modified model with ignore_mismatched_size
  model = AutoModelForCausalLM.from_pretrained(local_model_path,
                                        torch_dtype=torch.bfloat16,
                                        ignore_mismatched_sizes=True #This required for pushing the pruned model
                                        )

  #Push model to hub
  model.push_to_hub(repo_name)

  #Push the tokenizer to the hub
  tokenizer.push_to_hub(repo_name)

  print(f"Model and tokenizer pushed successfully to {repo_name}")

#Path to my local model is "modified_model_path"
local_model_path = modified_model_path

repository_name = f"abhinavv3/{local_model_path}"

original_model_name = "HuggingFaceTB/SmolLM-135M-Instruct"

#push the model
push_model_to_hub(local_model_path, original_model_name, repository_name)





